---
title: "Klasifikasi dan Diskriminan"
author: "Antonito HC"
date: "January 17, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

    #Deskripsi Data
Data yang digunakan adalah data sekunder yang diambil yang diambil dari Badan Pusat Satistik(BPS)mengenai jumlah pencari kerja, Migrasi dan lapangan kerja pada 9 sektor di Indonesia
```{r}
setwd("E:/ADI BUANA/DSC")
data1 <- read.csv("evaluasi.csv")
namapro <- read.csv("prov.csv")
rownames(data1) <- namapro$propinsi
```


### Data Set

```{r}
library(DT)
datatable(data1)

```



Variabel yang digunakan adalah sebagai berikut


Variabel | Keterangan
--------- | ----------
X1  | Pencari Kerja Laki-laki
X2  | Pencari Kerja Perempuan
X3  | Migrasi tahun 2015
X4  | Pertanian
X5  | Pertambangan Penggalian
X6  | Industri Pengolahan
X7  | Listrik Gas Air Bersih
X8  | Bangunan Konstruksi
X9  | Perdagangan Hotel Restoran
X10 | Pengangkutan
X11 | BankLembaga Keuangan
X12 | Jasajasa

####Rename Nama Variabel untuk memudahkan dalam analisis
```{r}
colnames(data1) <- c('X1', 'X2', 'X3', 'X4', 
                     'X5', 'X6', 'X7', 'X8', 
                     'X9', 'X10', 'X11', 'X12')
```

#### Explorasi data ringkasan 5 numerik data (Summary)
```{r}
summary(data1)
```
Dari output diatas dapat dilihat bahwa rata-rata pencari kerja laki-laki adalah 23224 sedangkan perempuan adalah 22644. Untuk migrasi tahun 2015, rata-rata sebanyak 454608, dekimian untukvariabel lainnya.

#### Standarisasi data
Standarisasi dilakukan untuk menghindari varians yang tinggi  
```{r}
df <- scale(data1) 
head(df)
```


***********************************************************************                                                                     

#K-Means Cluster
K-means Cluster merupakan salah satu metode non hirarki. Dalam kasus ini telah ditentukan jumlah klaster yang akan ditentukan. Jumlah klaster dalam kasus ini sebanyak 3

Berikut adalah library yang digunakan dalam K-means cluster
```{r}
library(cluster)
library(dendextend)
library(factoextra)
```

####Penentuan jumlah klaster

```{r}
set.seed(123)
km <- kmeans(df, 3, nstart = 25)
print(km)
```
  
  Berdasarkan Output diatas dalam dilihat bahwa jumlah klaster yang dibentuk sebanayak 3 klaster. Klaster 1 memiliki 6 anggota, klaster 2 memilki 4 anggota sedangkan klaster memiliki 24 anggota. untuk mengetahui masing-masing anggota klaster dapat dilihat pada visualisasi dibawah ini.

#### Visualisasi anggota klaster
```{r}
fviz_cluster(km, data = df,
             palette = c("green", "blue", "red"),
             ellipse.type = "euclid", # Concentration ellipse
             star.plot = T, # Add segments from centroids to items
             repel = T, # Avoid label overplotting (slow)
             ggtheme = theme_minimal()
)
```

Dari visualisasi diatas dapat dilihat bahwa anggota pada klaster pertama adalah DKI Jakarta, Banten, Kalimantan Timur, Kalimantan Utara, Kepulauan Riau dan Papua. Untuk anggota Klaster, anggotanya adalah Jawa Tengah, Jawa Timur, Jawa Barat dan Sulawesi Selatan. Sedangkan sisanya masuk dalam klaster ketiga.



```{r}
data.baru1 <- cbind(df, cluster=km$cluster) 
head(data.baru1)

dat <- as.data.frame(data.baru1)
```

***

Sebelum melakukan analisis diskriminan perlu mengecek asumsi analisis diskriminan. Asumsi diskriminan diantaranya adalah tidak ada korelasi yang tinggi antara variabel bebas(Multikolinearitas), homogenitas matriks kovarians. 

### Asumsi Multikolinearitas
Salah satu cara untuk mengecek ada atau tidaknya multikolinearitas adalah dengan melihat nilai korelasi antara variabel bebas. Jika nilai korelasi antara variabel bebas diatas 0.70 berarti diduga ada multikolinearita.
```{r}
round(cor(data1),3)

```

```{r}
library(psych)
library(corrplot)
```

```{r}
pairs.panels(data1)

```

Dari output dan visualisasi diatas dapat dilihat bahwa ada beberapa nilai korelasi yang diatas 0.70 sehingga terjadi multikolinearitas dalam data tersebut. Untuk mengatasi masalah multikolinearitas maka akan melakukan dengan metode _Principal Component Analysis_ (PCA)

#_Principal Component Analysis_
### Uji Asumsi KMO
Asumsi yang harus dipenuhi dalam PCA adalah kecukupan data. Uji kecukupan dalam analisis ini menggunakan uji Kasier Kayer Olkin (KMO). Sedangkan  uji korelasi yang digunakan untuk mengetahui apakah terdapat hubungan antar variabel adalah uji Barrlett
```{r}
kmo <- function(x)
{
  x <- subset(x, complete.cases(x)) # Omit missing values
  r <- cor(x) # Correlation matrix
  r2 <- r^2 # Squared correlation coefficients
  i <- solve(r) # Inverse matrix of correlation matrix
  d <- diag(i) # Diagonal elements of inverse matrix
  p2 <- (-i/sqrt(outer(d, d)))^2 # Squared partial correlation coefficients
  diag(r2) <- diag(p2) <- 0 # Delete diagonal elements
  KMO <- sum(r2)/(sum(r2)+sum(p2))
  MSA <- colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

kmo(df)
```
Diketahui nilai KMO adalah 0.758 atau 75.5% sehingga data telah cukup untuk melakukan analisis faktor. Nilai MSA untuk seluruh variabel bebas diatas 0.5 sehingga analisis faktor dapat dilakukan.

```{r}
Bartlett.spher.test <- function(x)
{
  method <- "Bartlett's test of sphericity"
  data.name <- deparse(substitute(x))
  x <- subset(x, complete.cases(x)) # Omit missing values
  n <- nrow(x)
  p <- ncol(x)
  chisq <- (1-n+(2*p+5)/6)*log(det(cor(x)))
  df <- p*(p-1)/2
  p.value <- pchisq(chisq, df, lower.tail=FALSE)
  names(chisq) <- "X-squared"
  names(df) <- "df"
  return(structure(list(statistic=chisq, parameter=df, p.value=p.value,
                        method=method, data.name=data.name), class="htest"))
}
Bartlett.spher.test(df)
```
Ditunjukkan juga nilai  P-value Barltlett test didapatkan sebesar 0.000 yang sangat kecil dari taraf signifikan sebesar 0.05, dari pernyataan tersebut dapat diputuskan menolak H~0~ sehingga dapat disimpulkan  untuk dilanjutkan analisis berikutnya.


##_Principal Component Analysis_
Analisis komponen utama digunakan untuk mengelompokkan variabel-variabel yang memiliki korelasi yang tinggi. Berikut adalah analisis dan pembahasan untuk analisis komponen utama.


#### Visualisasi Plot

```{r}
plot(eigen(cor(df))$value,ylab = "variance", xlab = "component")
lines(eigen(cor(df))$values,)
```

  Dari _Scree Plot_ dapat dilihat bahwa dari titik pertama ke titik kedua menunjukkan garis yang turun secara curam dan dari titik kedua ke titik ketiga turun secara landai. Namun dalam penentuan komponen dengan visualisai plot mungkin terlalu objektif sehingga untuk mengetahui berapa komponen yang akan terbentuk maka akan melihat niai _Eigen Value_ yang kurang sama dengan satu (>=1). Hasil _Eigen Value_ ditunjukkan pada sebagai berikut

####Penentuan jumlah komponen berdasarkan _eigen value_
```{r}
pc <- prcomp(df,
             center = T,
             scale. = T)
summary(pc)
```
Nilai eigenvalue diatas menunjukkan dasar pembentuk faktor baru yang terletak pada komponen pertama, kedua dan ketiga dimana nilai eigen value-nya lebih dari satu. Nilai total eigen value pada komponen pertama sebesar 2.31 komponen kedua sebesar 1.57 dan komponen ketiga sebesar 1.03. Hal tersebut mempunyai arti bahwa terdapat 3 komponen baru yang terbentuk. Ketiga komponen baru tersebut memiliki nilai kumulatif varians sebesar 74.35 persen yang mempunyai arti bahwa ketiga komponen baru yang terbentuk mampu menjelaskan 74.35 persen variabilitas dari data yang dihasilkan.


#### Mengambil 3 komponen baru
```{r}
pca <- round(pc$x[,1:3],3)
head(pca)

```

```{r}
data.baru1 <- cbind(df, cluster=km$cluster)
klaster <- data.baru1[,13]
head(klaster)
```


#### Mengabungan data PCA dan  data Cluster
Menggabungan PCA dan Cluster dengan membuat data frame baru untuk analisis diskriminan 
```{r}
dsk <- cbind(pca,klaster)
head(dsk)
```

# ANALISIS DISKRIMINAN
```{r}
data.disk <- as.data.frame(dsk)
attach(data.disk)
```


#### Data Untuk Analisis Diskriminan
```{r}
datatable(data.disk)
```


### Melihat Struktur data
Dalam analisis diskriminan variabel dependen(Y) harus kategori(non metrik). untuk melihat struktur bisa 

```{r}
str(data.disk)
```
Dari output diatas variavel Y (klaster), Software R baca numerik. oleh karena itu harus mengubah struktur datanya. berikut adalah caranya.

```{r}
data.disk$klaster <- as.factor(data.disk$klaster)
str(data.disk)
```
Dapat dilihat bahwa variabel Y (klaster) sudah jadi variabel kategori.

library untuk analisis diskriminan
```{r}
library(psych)
library(MASS)
```

```{r}
pairs.panels(data.disk[1:3],
             gap=0,
             bg=c("red","blue","green")[data.disk$klaster],
             pch = 21)
```
Plot diatas menunjukkan bahwa antara variabel bebas memiliki nilai korelasi 0 sehingga tidak terjadi multikolinearitas.


```{r}
library(biotools)
boxM(data.disk[,1:3], data.disk$klaster)
```


## Data Partisi
Data partisi yang dimaksud disini adalah membuat data training dan data testing. data training digunakan untuk membuat model diskriminan sedangkan data testing digunakan untuk mengvaldasi model atau sejauh mana model tersebut bisa digunakan untuk memprediksi kasus baru. Karena data sampel  terlalu kecil sehingga data disini saya ambil data random 80% dari data untuk training sedangkan 20% untuk testing. berikut adalah hasilnya

```{r}
set.seed(222)
ind <- sample(2, nrow(data.disk),
              replace = T,
              prob = c(0.8,0.2))

training <- data.disk[ind==1,]
testing <- data.disk[ind==2,]
```

### Model Diskriminan
```{r}
model <- lda(klaster ~., data = training)
model
```
Output diatas menunjukkan model diskriminan yang ditunjukkan pada _Coefficients of linear discriminants_.
Model untuk LD1:

First  discriminant function is a linear combination of three variables (PC1, PC2, PC3)

Fungsi diskriminan dapat ditulis sebagai berikut

LD1 = 0.2411362 PC1 - 1.3515285PC2 + 0.3603853 PC3

Model diatas dapat diinterpretasi bahwa variabel PC1 dan PC3 memiliki yang positif dan searah dengan fungsi diskriminan. Jika PC1 dan PC3 naik satu satuan maka akan menaikan fungsi diskriminan sedangkan PC2 memiliki koefesien yang negatif artinya variabel PC2 naik maka akan menerunkan nilai skor diskriminan.

#### Bi-plot
```{r}
library(devtools)
library(ggord)
ggord(model, training$klaster)
```

Bi-plot diatas membantu kita  untuk memahami fungsi diskriminan yang terbentuk. Terlihat bahwa pada Fungsi LD1 variabel PC1 dan PC3 berada pada daerah positif sedangkan PC2 berada pada daerah negatif. Pada fungsi LD2 terlihat bahwa PC1, PC2 dan PC3 berada pada daerah yang negatif. 

#### Confusion Matrix and Accuracy untuk data training
```{r}
p.training <- predict(model, training)$class
tabel.training <- table(Actual = training$klaster, predicted =  p.training)
tabel.training
```
Terlihat bahwa hanya ada satu data yang salah klasifikasi. Data tersebut awalnya masuk klaster 1 tetapi model diskriminan prediksi masuk klaster 3.

#### Accuracy data training
```{r}
sum(diag(tabel.training))/sum(tabel.training)
```
Berdasarkan output diatas dapat dilihat bahwa akurasi untuk data training sebesar 0.9629 atau tingkat akurasi sangat tinggi yaitu mencapai 96.19% 


#### Confusion Matrix and Accuracy untuk data training
```{r}
p.testing <- predict(model, testing)$class
tabel.testing <- table(Actual = testing$klaster, predicted =  p.testing)
tabel.testing
```

#### Accuracy data testing
```{r}
sum(diag(tabel.testing))/sum(tabel.testing)
```
Terlihat bahwa akurasi untuk data testing sebesar 100%.


***
***
***

TERMINAR ESTE PROJETO.

VAMOS CONTINUAR AO OUTRO PROJECTO














